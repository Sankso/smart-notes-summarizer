{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#  Install Only Missing Dependencies\n!pip install -q peft rouge-score\n!pip install -q evaluate\n\n\n# STEP 2: Import Libraries\nimport torch\nfrom transformers import (\n    AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq,\n    TrainingArguments, Trainer\n)\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nimport evaluate\nimport shutil\n\n# STEP 3: Load Base Model + Tokenizer\nmodel_name = \"google/flan-t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n#  STEP 4: Apply LoRA Config\nlora_config = LoraConfig(\n    r=16,                      \n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.1,          \n    bias=\"none\",\n    task_type=\"SEQ_2_SEQ_LM\"\n)\nmodel = get_peft_model(model, lora_config)\n\n#  STEP 5: Load Dataset (subset for demo)\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:50000]\")\n\n#  STEP 6: Preprocess Dataset\ndef preprocess(examples):\n    inputs = examples[\"article\"]\n    targets = examples[\"highlights\"]\n    model_inputs = tokenizer(\n        inputs, max_length=512, truncation=True\n    )\n    labels = tokenizer(\n        targets, max_length=150, truncation=True\n    )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(\n    preprocess, batched=True, remove_columns=[\"article\", \"highlights\", \"id\"]\n)\n\n#  STEP 7: Split train/test\nsplit_dataset = tokenized_dataset.train_test_split(test_size=0.1)\ntrain_ds = split_dataset['train']\neval_ds = split_dataset['test']\n\n#  STEP 8: Setup Data Collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n#  STEP 9: Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/results\",\n    eval_strategy=\"steps\",     \n    eval_steps=500,                   \n    logging_steps=50,\n    learning_rate=3e-4,                \n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=4,                \n    weight_decay=0.01,\n    save_total_limit=1,\n    fp16=True,                         \n    save_strategy=\"epoch\",\n    logging_dir=\"/kaggle/working/logs\",\n    report_to=\"none\"                   \n)\n\n#  STEP 10: Trainer Setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\n#  STEP 11: Train Model\ntrainer.train()\n\n#  STEP 12: Save Fine-Tuned Model + Tokenizer\nsave_path = \"/kaggle/working/finetuned_model\"\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\nprint(\"✅ Model fine-tuned and saved to:\", save_path)\n\n#  STEP 13: Zip model for download\nzip_path = \"/kaggle/working/finetuned_model.zip\"\nshutil.make_archive(\"/kaggle/working/finetuned_model\", 'zip', save_path)\nprint(\"✅ Model zipped successfully!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-16T03:07:18.851061Z","iopub.execute_input":"2025-09-16T03:07:18.851554Z","iopub.status.idle":"2025-09-16T06:29:16.044478Z","shell.execute_reply.started":"2025-09-16T03:07:18.851532Z","shell.execute_reply":"2025-09-16T06:29:16.043683Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/tmp/ipykernel_36/3410654146.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11252' max='11252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11252/11252 3:21:46, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"✅ Model fine-tuned and saved to: /kaggle/working/finetuned_model\n✅ Model zipped successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Link to download the zipped model\nzip_path = \"/kaggle/working/finetuned_model.zip\"\nFileLink(zip_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T06:32:25.097958Z","iopub.execute_input":"2025-09-16T06:32:25.098251Z","iopub.status.idle":"2025-09-16T06:32:25.104104Z","shell.execute_reply.started":"2025-09-16T06:32:25.098221Z","shell.execute_reply":"2025-09-16T06:32:25.103440Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/finetuned_model.zip","text/html":"<a href='/kaggle/working/finetuned_model.zip' target='_blank'>/kaggle/working/finetuned_model.zip</a><br>"},"metadata":{}}],"execution_count":8}]}