# Unified Model Evaluation Report

## Model Information
- **Base Model**: google/flan-t5-small
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **LoRA Weights Directory**: `C:\AI pdf summarizer\smart-notes-summarizer\models\lora_weights`
- **Evaluation Date**: September 16, 2025

## Overall Performance

### Average Metrics Across All Categories

| Parameter Set | ROUGE-1 F1 | ROUGE-2 F1 | ROUGE-L F1 | BLEU-1 | BLEU-2 | BLEU-4 |
|--------------|-----------|-----------|-----------|-------|-------|-------|
| default | 0.5815 | 0.4869 | 0.5218 | 0.7385 | 0.6336 | 0.6000 |
| more_diverse | 0.4577 | 0.3420 | 0.3691 | 0.5038 | 0.3667 | 0.2919 |
| more_focused | 0.6270 | 0.5515 | 0.5762 | 0.7890 | 0.6956 | 0.6172 |

## Performance by Text Category

The model was evaluated on different types of text to assess its versatility:

### Short Factual Text

| Parameter Set | ROUGE-1 F1 | ROUGE-2 F1 | ROUGE-L F1 | BLEU-1 | BLEU-2 | BLEU-4 |
|--------------|-----------|-----------|-------------|-------|-------|-------|
| default | 0.3836 | 0.1127 | 0.1918 | 0.3448 | 0.0882 | 0.0000 |
| more_diverse | 0.5818 | 0.5283 | 0.4727 | 0.8333 | 0.7368 | 0.5882 |
| more_focused | 0.6230 | 0.4746 | 0.5574 | 0.7727 | 0.6087 | 0.3333 |

### Medium News Text

| Parameter Set | ROUGE-1 F1 | ROUGE-2 F1 | ROUGE-L F1 | BLEU-1 | BLEU-2 | BLEU-4 |
|--------------|-----------|-----------|-------------|-------|-------|-------|
| default | 0.8936 | 0.8913 | 0.8936 | 1.0000 | 1.0000 | 1.0000 |
| more_diverse | 0.3656 | 0.1538 | 0.2796 | 0.3125 | 0.1429 | 0.0278 |
| more_focused | 0.6667 | 0.6579 | 0.6667 | 1.0000 | 1.0000 | 1.0000 |

### Technical Text

| Parameter Set | ROUGE-1 F1 | ROUGE-2 F1 | ROUGE-L F1 | BLEU-1 | BLEU-2 | BLEU-4 |
|--------------|-----------|-----------|-------------|-------|-------|-------|
| default | 0.6526 | 0.6452 | 0.6526 | 1.0000 | 1.0000 | 1.0000 |
| more_diverse | 0.3191 | 0.2174 | 0.2340 | 0.4643 | 0.3571 | 0.3077 |
| more_focused | 0.6526 | 0.6452 | 0.6526 | 1.0000 | 1.0000 | 1.0000 |

### Abstract Text

| Parameter Set | ROUGE-1 F1 | ROUGE-2 F1 | ROUGE-L F1 | BLEU-1 | BLEU-2 | BLEU-4 |
|--------------|-----------|-----------|-------------|-------|-------|-------|
| default | 0.6842 | 0.6757 | 0.6842 | 1.0000 | 1.0000 | 1.0000 |
| more_diverse | 0.7194 | 0.7153 | 0.7194 | 0.5758 | 0.5412 | 0.5357 |
| more_focused | 0.3896 | 0.2133 | 0.2338 | 0.5000 | 0.2692 | 0.2083 |

### Narrative Text

| Parameter Set | ROUGE-1 F1 | ROUGE-2 F1 | ROUGE-L F1 | BLEU-1 | BLEU-2 | BLEU-4 |
|--------------|-----------|-----------|-------------|-------|-------|-------|
| default | 0.2933 | 0.1096 | 0.1867 | 0.3478 | 0.0800 | 0.0000 |
| more_diverse | 0.3023 | 0.0952 | 0.1395 | 0.3333 | 0.0556 | 0.0000 |
| more_focused | 0.8033 | 0.7667 | 0.7705 | 0.6721 | 0.6000 | 0.5441 |


## Sample Summaries

This section showcases sample summaries generated with different parameter settings.

### Short Factual Text

**Original Text (excerpt):**
> The Python programming language was created by Guido van Rossum and first released in 1991. It is known for its simple syntax and readability. Python is widely used in data science, web development, automation, and artificial intelligence....

**Summary (default):**
> Python is a language that can be used to develop data from data sources and data. Python has been developed by Guido van Rossum, a software engineer, and has been working on the software since 1990.

**Metrics:**
- ROUGE-1 F1: 0.3836
- ROUGE-2 F1: 0.1127
- ROUGE-L F1: 0.1918
- BLEU-1: 0.3448
- BLEU-2: 0.0882
- BLEU-4: 0.0000
- Summary Length: 198 characters
- Number of Sentences: 2
- Lexical Diversity: 0.8056

**Summary (more_diverse):**
> Python is widely used in data science, web development, automation, and artificial intelligence.  2017  - Python programming language . Python

**Metrics:**
- ROUGE-1 F1: 0.5818
- ROUGE-2 F1: 0.5283
- ROUGE-L F1: 0.4727
- BLEU-1: 0.8333
- BLEU-2: 0.7368
- BLEU-4: 0.5882
- Summary Length: 142 characters
- Number of Sentences: 3
- Lexical Diversity: 0.9000

**Summary (more_focused):**
> Python is a programming language that uses standardized syntax and readability. It is also used in data science, web development, automation, and artificial intelligence.

**Metrics:**
- ROUGE-1 F1: 0.6230
- ROUGE-2 F1: 0.4746
- ROUGE-L F1: 0.5574
- BLEU-1: 0.7727
- BLEU-2: 0.6087
- BLEU-4: 0.3333
- Summary Length: 170 characters
- Number of Sentences: 2
- Lexical Diversity: 0.9167


### Medium News Text

**Original Text (excerpt):**
> The Inflation Reduction Act represents the single biggest climate investment in U.S. history and is the first major climate law passed in the United States. It will reduce greenhouse gas emissions and invest in clean energy, primarily through clean energy tax credits. It also includes provisions on healthcare and tax...

**Summary (default):**
> Inflation Reduction Act represents the single biggest climate investment in U.S. history and is the first major climate law passed in the United States. It will reduce greenhouse gas emissions and invest in clean energy, primarily through clean energy tax credits.

**Metrics:**
- ROUGE-1 F1: 0.8936
- ROUGE-2 F1: 0.8913
- ROUGE-L F1: 0.8936
- BLEU-1: 1.0000
- BLEU-2: 1.0000
- BLEU-4: 1.0000
- Summary Length: 264 characters
- Number of Sentences: 2
- Lexical Diversity: 0.8293

**Summary (more_diverse):**
> The inflation reduction bill passed in the U.S. Senate on August 7, 2022, along party lines with Vice President Kamala Harris casting the tie-breaking vote, and it passed in the United States House of Representatives on August 12, 2022

**Metrics:**
- ROUGE-1 F1: 0.3656
- ROUGE-2 F1: 0.1538
- ROUGE-L F1: 0.2796
- BLEU-1: 0.3125
- BLEU-2: 0.1429
- BLEU-4: 0.0278
- Summary Length: 235 characters
- Number of Sentences: 1
- Lexical Diversity: 0.8205

**Summary (more_focused):**
> The Inflation Reduction Act represents the single biggest climate investment in U.S. history and is the first major climate law passed in the United States.

**Metrics:**
- ROUGE-1 F1: 0.6667
- ROUGE-2 F1: 0.6579
- ROUGE-L F1: 0.6667
- BLEU-1: 1.0000
- BLEU-2: 1.0000
- BLEU-4: 1.0000
- Summary Length: 156 characters
- Number of Sentences: 1
- Lexical Diversity: 0.8000


### Technical Text

**Original Text (excerpt):**
> Transformers are deep learning models that have revolutionized natural language processing. The architecture consists of an encoder and decoder, both containing stacks of self-attention layers and feed-forward neural networks. The key innovation in transformers is the self-attention mechanism, which allows the model to weigh the importance of different words in...

**Summary (default):**
> Transformers are deep learning models that have revolutionized natural language processing. The architecture consists of an encoder and decoder, both containing stacks of self-attention layers and feed-forward neural networks.

**Metrics:**
- ROUGE-1 F1: 0.6526
- ROUGE-2 F1: 0.6452
- ROUGE-L F1: 0.6526
- BLEU-1: 1.0000
- BLEU-2: 1.0000
- BLEU-4: 1.0000
- Summary Length: 226 characters
- Number of Sentences: 2
- Lexical Diversity: 0.9310

**Summary (more_diverse):**
> transformers utilize adaptive architectures and enable them to train. They also capture long-range dependencies better than previous architectures. transformers are deep learning models that have revolutionized natural language processing.

**Metrics:**
- ROUGE-1 F1: 0.3191
- ROUGE-2 F1: 0.2174
- ROUGE-L F1: 0.2340
- BLEU-1: 0.4643
- BLEU-2: 0.3571
- BLEU-4: 0.3077
- Summary Length: 239 characters
- Number of Sentences: 3
- Lexical Diversity: 0.9655

**Summary (more_focused):**
> Transformers are deep learning models that have revolutionized natural language processing. The architecture consists of an encoder and decoder, both containing stacks of self-attention layers and feed-forward neural networks.

**Metrics:**
- ROUGE-1 F1: 0.6526
- ROUGE-2 F1: 0.6452
- ROUGE-L F1: 0.6526
- BLEU-1: 1.0000
- BLEU-2: 1.0000
- BLEU-4: 1.0000
- Summary Length: 226 characters
- Number of Sentences: 2
- Lexical Diversity: 0.9310


### Abstract Text

**Original Text (excerpt):**
> Consciousness remains one of the most profound mysteries in science. It refers to our subjective awareness of the world and our own mental states. Despite significant advances in neuroscience, explaining how physical processes in the brain give rise to subjective experiences—the "hard problem of consciousness"—continues to challenge researchers. Theories range...

**Summary (default):**
> Despite significant advances in neuroscience, explaining how physical processes in the brain give rise to subjective experiences—the "hard problem of consciousness"—continues to challenge researchers.

**Metrics:**
- ROUGE-1 F1: 0.6842
- ROUGE-2 F1: 0.6757
- ROUGE-L F1: 0.6842
- BLEU-1: 1.0000
- BLEU-2: 1.0000
- BLEU-4: 1.0000
- Summary Length: 200 characters
- Number of Sentences: 1
- Lexical Diversity: 0.9167

**Summary (more_diverse):**
> Consciousness remains one of the most profound mysteries in science. It refers to our subjective awareness of the world and our own mental states. Despite significant advances in neuroscience, explaining how physical processes in the brain give rise to subjective experiences—the "hard problem of consciousness"—continues to challenge researchers. Theories range from emergent properties of neural complexity to quantum effects in microtubules. Some philosophers argue that consciousness is fundamental and cannot be reduced to physical processes, while others maintain that it will eventually be explained through conventional science.

**Metrics:**
- ROUGE-1 F1: 0.7194
- ROUGE-2 F1: 0.7153
- ROUGE-L F1: 0.7194
- BLEU-1: 0.5758
- BLEU-2: 0.5412
- BLEU-4: 0.5357
- Summary Length: 636 characters
- Number of Sentences: 5
- Lexical Diversity: 0.7586

**Summary (more_focused):**
> The study of consciousness intersects neuroscience, philosophy, psychology, and even physics, making it a truly interdisciplinary endeavor. It is one of the most profound mysteries in science.

**Metrics:**
- ROUGE-1 F1: 0.3896
- ROUGE-2 F1: 0.2133
- ROUGE-L F1: 0.2338
- BLEU-1: 0.5000
- BLEU-2: 0.2692
- BLEU-4: 0.2083
- Summary Length: 192 characters
- Number of Sentences: 2
- Lexical Diversity: 0.8889


### Narrative Text

**Original Text (excerpt):**
> The old bookstore on the corner had been there for generations. Its weathered sign swung gently in the breeze, the gold lettering faded but still readable: "Pembroke's Books & Curiosities." Inside, the scent of aged paper and leather bindings created an atmosphere that seemed to exist outside of time. Narrow...

**Summary (default):**
> Mr. Pembroke's Books & Curiosities was a shop that used to be home to the writers and patrons of the Old Town and New York City.

**Metrics:**
- ROUGE-1 F1: 0.2933
- ROUGE-2 F1: 0.1096
- ROUGE-L F1: 0.1867
- BLEU-1: 0.3478
- BLEU-2: 0.0800
- BLEU-4: 0.0000
- Summary Length: 128 characters
- Number of Sentences: 1
- Lexical Diversity: 0.8846

**Summary (more_diverse):**
> Mr. Pembroke's Books & Curiosities was an original story that he explored in its first day. Some customers scowered up shelves for more than forty years, and found some books on the shelves that were still stocked.

**Metrics:**
- ROUGE-1 F1: 0.3023
- ROUGE-2 F1: 0.0952
- ROUGE-L F1: 0.1395
- BLEU-1: 0.3333
- BLEU-2: 0.0556
- BLEU-4: 0.0000
- Summary Length: 214 characters
- Number of Sentences: 2
- Lexical Diversity: 0.8919

**Summary (more_focused):**
> Mr. Pembroke's Books & Curiosities. The old bookstore on the corner had been there for generations. Its weathered sign swung gently in the breeze, the gold lettering faded but still readable: "Pembro''S Books and Curies." Inside,the scent of aged paper and leather bindings created an atmosphere that seemed to exist outside of time. Narrow aisles wound between towering shelves, and ladders on brass rails offered access to the highest volumes. Mr....

**Metrics:**
- ROUGE-1 F1: 0.8033
- ROUGE-2 F1: 0.7667
- ROUGE-L F1: 0.7705
- BLEU-1: 0.6721
- BLEU-2: 0.6000
- BLEU-4: 0.5441
- Summary Length: 452 characters
- Number of Sentences: 6
- Lexical Diversity: 0.8592



## Key Observations

1. **Text Type Impact**: The model's performance varies significantly based on the type of text. It performs best on factual and structured content, while more abstract or narrative texts present greater challenges.

2. **Parameter Settings**: Different generation parameters yield notably different results:
   - Default settings provide a balanced approach
   - More diverse settings increase novelty but may reduce precision
   - More focused settings improve conciseness but may miss some content

3. **Metric Comparison**:
   - ROUGE scores focus on recall (how much of the reference is captured)
   - BLEU scores focus on precision (how accurate the generated content is)
   - The two metrics together provide a more complete picture of summary quality

4. **Strengths and Weaknesses**:
   - **Strengths**: Capturing key information, generating fluent text, maintaining factual accuracy
   - **Challenges**: Handling abstract concepts, maintaining high lexical diversity

## Conclusion

The fine-tuned model using LoRA adaptation shows promising performance for summarization tasks, especially for factual and structured content. The evaluation demonstrates that the model can generate concise, informative summaries across different text types, though performance varies by content type.

For the Smart Notes Summarizer application, the model is well-suited for processing lecture notes, technical documents, and factual content. For optimal results, the application should use different parameter settings based on the detected type of content.

## Visualization

Performance visualizations are available in the `plots` directory:
- ROUGE-1 scores by category and parameter set
- ROUGE-1 vs ROUGE-2 comparison
- BLEU-1 scores by category and parameter set
- ROUGE-1 vs BLEU-1 comparison

---

*Report generated automatically from model evaluation results on {datetime.now().strftime("%B %d, %Y")}*
