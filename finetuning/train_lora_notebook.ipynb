{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac03640",
   "metadata": {},
   "source": [
    "# T5 Model Fine-tuning with LoRA for Summarization\n",
    "\n",
    "This notebook demonstrates how to fine-tune a T5 model with LoRA (Low-Rank Adaptation) for text summarization tasks. The process uses parameter-efficient fine-tuning to minimize computational requirements while achieving good results.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA is a technique that freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture. This significantly reduces the number of trainable parameters.\n",
    "\n",
    "## Steps in this Notebook:\n",
    "1. Install required dependencies\n",
    "2. Load the base T5 model\n",
    "3. Apply LoRA configuration\n",
    "4. Load and preprocess the CNN/DailyMail dataset\n",
    "5. Train the model\n",
    "6. Save and export the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5978784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Install Only Missing Dependencies\n",
    "!pip install -q peft rouge-score\n",
    "!pip install -q evaluate\n",
    "\n",
    "# STEP 2: Import Libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc88f8",
   "metadata": {},
   "source": [
    "## Load Base Model & Apply LoRA Configuration\n",
    "\n",
    "We'll use the google/flan-t5-small model as our base model, which is a good balance between quality and computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Load Base Model + Tokenizer\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "#  STEP 4: Apply LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                      # Rank of the update matrices\n",
    "    lora_alpha=32,             # Scaling factor\n",
    "    target_modules=[\"q\", \"v\"], # Which modules to apply LoRA to\n",
    "    lora_dropout=0.1,          # Dropout probability for LoRA layers\n",
    "    bias=\"none\",               # Add bias to LoRA layers\n",
    "    task_type=\"SEQ_2_SEQ_LM\"   # Task type\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters info\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc096b2",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset\n",
    "\n",
    "We'll use the CNN/DailyMail dataset, which contains news articles paired with multi-sentence summaries. For efficient training, we'll use a subset of 50,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e29e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 5: Load Dataset (subset for demo)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:50000]\")\n",
    "print(f\"Dataset loaded: {len(dataset)} examples\")\n",
    "print(\"\\nSample example:\")\n",
    "print(f\"Article excerpt: {dataset[0]['article'][:200]}...\")\n",
    "print(f\"Summary: {dataset[0]['highlights']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db08417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 6: Preprocess Dataset\n",
    "def preprocess(examples):\n",
    "    inputs = examples[\"article\"]\n",
    "    targets = examples[\"highlights\"]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        targets, max_length=150, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess, batched=True, remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "\n",
    "#  STEP 7: Split train/test\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_ds = split_dataset['train']\n",
    "eval_ds = split_dataset['test']\n",
    "\n",
    "print(f\"Training set: {len(train_ds)} examples\")\n",
    "print(f\"Validation set: {len(eval_ds)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0eb7e",
   "metadata": {},
   "source": [
    "## Configure Training Parameters\n",
    "\n",
    "Now we'll set up the training configuration and data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfbad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 8: Setup Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "#  STEP 9: Training Arguments\n",
    "output_dir = \"./results\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"steps\",      # Evaluation strategy: steps or epoch\n",
    "    eval_steps=500,             # Evaluation steps  \n",
    "    logging_steps=50,           # Logging steps\n",
    "    learning_rate=3e-4,         # Learning rate     \n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=4,         # Number of training epochs       \n",
    "    weight_decay=0.01,          # Weight decay\n",
    "    save_total_limit=1,         # Limit the total amount of checkpoints\n",
    "    fp16=True,                  # Use mixed precision (if available)       \n",
    "    save_strategy=\"epoch\",      # Save strategy\n",
    "    logging_dir=\"./logs\",       # Directory for logs\n",
    "    report_to=\"none\"            # No reporting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353c969",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we're ready to set up the trainer and start training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3547cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 10: Trainer Setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "#  STEP 11: Train Model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b82adf",
   "metadata": {},
   "source": [
    "## Save and Export the Model\n",
    "\n",
    "Finally, we'll save the fine-tuned model and prepare it for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 12: Save Fine-Tuned Model + Tokenizer\n",
    "save_path = \"./finetuned_model\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"✅ Model fine-tuned and saved to:\", save_path)\n",
    "\n",
    "#  STEP 13: Zip model for download (optional)\n",
    "try:\n",
    "    zip_path = \"./finetuned_model.zip\"\n",
    "    shutil.make_archive(\"./finetuned_model\", 'zip', save_path)\n",
    "    print(\"✅ Model zipped successfully to:\", zip_path)\n",
    "except Exception as e:\n",
    "    print(f\"Could not create zip file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a6247",
   "metadata": {},
   "source": [
    "## Test the Fine-tuned Model\n",
    "\n",
    "Let's test our model on a sample article to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd245d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Get a test article\n",
    "test_article = dataset[10000]['article']\n",
    "print(\"Original article excerpt:\")\n",
    "print(test_article[:500] + \"...\\n\")\n",
    "\n",
    "print(\"Original summary:\")\n",
    "print(dataset[10000]['highlights'] + \"\\n\")\n",
    "\n",
    "# Generate summary with our fine-tuned model\n",
    "inputs = tokenizer(test_article, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "output = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"], \n",
    "    max_new_tokens=150, \n",
    "    min_length=30,\n",
    "    no_repeat_ngram_size=3\n",
    ")\n",
    "summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a1244",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've:\n",
    "1. Fine-tuned a T5 model using LoRA for summarization\n",
    "2. Used the CNN/DailyMail dataset for training\n",
    "3. Saved the model for later use\n",
    "\n",
    "The fine-tuned model can now be integrated into the Smart Notes Summarizer project."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
